{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GDS Assignment 1 \n",
    "Rasmus DÃ¼beck Kristensen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "I formulated a regular expression to filter CPR numbers and included a number of testcases to check that the correct centuries were returned, which they were. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person 1: 1234, 56: Born in century 1900\n",
      "Person 2: 4222, 11: Born in century 2000\n",
      "Person 3: 4222, 76: Born in century 1900\n",
      "Person 4: 5009, 11: Born in century 2000\n",
      "Person 5: 5009, 76: Born in century 1800\n",
      "Person 6: 9001, 11: Born in century 2000\n",
      "Person 7: 9001, 41: Born in century 1900\n"
     ]
    }
   ],
   "source": [
    "# Import regular expression module\n",
    "import re\n",
    "\n",
    "CPRdata = '''\n",
    "123456-1234\n",
    "1234561234\n",
    "483493-1234\n",
    "4834931234\n",
    "324567-1234\n",
    "3245671234\n",
    "984321-1234\n",
    "9843211234\n",
    "''' \n",
    "\n",
    "# Create regular expression\n",
    "pattern = re.compile(\"^([0-9]{2})([0-9]{2})([0-9]{2})-?([0-9]{4})\")\n",
    "\n",
    "for data_line in CPRdata.split('\\n'):\n",
    "    \n",
    "    match = pattern.match(data_line)\n",
    "    \n",
    "    if match: # If match is found, assign groups to variables\n",
    "        DD = int(match.group(1))\n",
    "        MM = int(match.group(2))\n",
    "        YY = int(match.group(3))\n",
    "        IIII = int(match.group(4))\n",
    "\n",
    "# Function to return century based on CPR number\n",
    "def return_century (IIII, YY):\n",
    "    if IIII in range(1, 3999) and YY in range(00,99):\n",
    "        return 1900\n",
    "    elif IIII in range(4000, 4999) and YY in range(00, 36):\n",
    "        return 2000\n",
    "    elif IIII in range(4000, 4999) and YY in range(37, 99):\n",
    "        return 1900\n",
    "    elif IIII in range(5000, 8999) and YY in range(00, 57):\n",
    "        return 2000\n",
    "    elif IIII in range(5000, 8999) and YY in range(58, 99):\n",
    "        return 1800\n",
    "    elif IIII in range(9000, 9999) and YY in range(00, 36):\n",
    "        return 2000\n",
    "    elif IIII in range(9000, 9999) and YY in range(37, 99):\n",
    "        return 1900\n",
    "    else: \n",
    "        return \"Invalid CPR number\"\n",
    "\n",
    "# Testcases (succesful):\n",
    "print(f\"Person 1: 1234, 56: Born in century {return_century(1234, 56)}\")\n",
    "print(f\"Person 2: 4222, 11: Born in century {return_century(4222, 11)}\")\n",
    "print(f\"Person 3: 4222, 76: Born in century {return_century(4222, 76)}\")\n",
    "print(f\"Person 4: 5009, 11: Born in century {return_century(5009, 11)}\")\n",
    "print(f\"Person 5: 5009, 76: Born in century {return_century(5009, 76)}\")\n",
    "print(f\"Person 6: 9001, 11: Born in century {return_century(9001, 11)}\")\n",
    "print(f\"Person 7: 9001, 41: Born in century {return_century(9001, 41)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1: I only considered the column \"content\" since I found it the most relevant for vocaublary analysis. URL, metadata and title could be misleading. \n",
    "\n",
    "I included a fairly long regex code for dates in order to find most dates in the set. I did not find all dates, but I did find a lot.\n",
    "\n",
    "I used whitespace_regex to replace multiple whitespace characters with a single space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jd/9drh55d12kd787my6ln93pbr0000gn/T/ipykernel_10870/3475012066.py:3: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(\"995,000_rows.csv\") # Load the data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"news_sample.csv\") # Load the data\n",
    "data['content']\n",
    "\n",
    "# Compile date-regex\n",
    "date_regex = re.compile(r\"\"\"\n",
    "\\b(\n",
    "    (?:\\d{1,2}[./-]\\d{1,2}[./-]\\d{2,4}) | # 01/01/2025, 1.1.2025, 1-1-25\n",
    "    (?:\\d{4}[./-]\\d{1,2}[./-]\\d{1,2}) | # 2025/01/01, 2025-1-1\n",
    "    (?:\\b(?:\\d{1,2})(?:st|nd|rd|th)?\\s+(?:of\\s+)?[A-Za-z]+\\s+\\d{4}\\b) |  # 1st January 2025, 2nd of February 2024\n",
    "    (?:\\b[A-Za-z]+\\s+(?:\\d{1,2})(?:st|nd|rd|th)?,?\\s+\\d{4}\\b) | # January 1st 2025, March 2nd, 2023\n",
    "    (?:\\b[A-Za-z]+\\s+\\d{4}\\b) # January 2025, March 2024\n",
    ")\\b\n",
    "\"\"\", re.VERBOSE | re.IGNORECASE)\n",
    "number_regex = re.compile(r'(\\d+)') # \"\\d+\" = One or more digits\n",
    "email_regex = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
    "url_regex = re.compile(r'\\b(?:http[s]?://|www\\.)[^\\s<>\"]+|www\\.[^\\s<>\"]+\\b')\n",
    "whitespace_regex = re.compile(r'\\s+')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(whitespace_regex, ' ', text) # Replace one or more whitespace characters with a single space\n",
    "    text = re.sub(date_regex, \"<DATE>\", text)\n",
    "    text = re.sub(url_regex, \"<URL>\", text)\n",
    "    text = re.sub(email_regex, \"<EMAIL>\", text)\n",
    "    text = re.sub(number_regex, \"<NUM>\", text)\n",
    "    return text\n",
    "\n",
    "for line in data['content']:\n",
    "    cleaned_line = clean_text((line))\n",
    "    print(cleaned_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: Since I couldn't find a \"clean-text\" method to filter dates, I reused the regex <date> code from above. \n",
    "Besides this, I simply set the relevant methods in the \"clean\" function to TRUE (including replacements) and thus cleaned the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleantext import clean\n",
    "\n",
    "# Replace dates with <DATE> \n",
    "data['date_replaced'] = data['content'].apply(lambda x: re.sub(date_regex, '<DATE>', x))\n",
    "\n",
    "# Use clean-text \n",
    "data['cleaned_content'] = data['date_replaced'].apply(lambda x: clean(x,\n",
    "    fix_unicode=False,\n",
    "    to_ascii=False,\n",
    "    lower=True,\n",
    "    no_line_breaks=True,\n",
    "    no_urls=True,\n",
    "    no_emails=True,\n",
    "    no_phone_numbers=False,\n",
    "    no_numbers=True,  \n",
    "    no_digits=True,\n",
    "    no_currency_symbols=False,\n",
    "    no_punct=False,\n",
    "    replace_with_url=\"<URL>\",\n",
    "    replace_with_email=\"<EMAIL>\",\n",
    "    replace_with_phone_number=\"<PHONE>\",\n",
    "    replace_with_number=\"<NUM>\",\n",
    "    replace_with_digit=\"<NUM>\",\n",
    "    replace_with_currency_symbol=\"<CUR>\",\n",
    "    lang=\"en\"\n",
    "))\n",
    "\n",
    "# Print result\n",
    "# for line in data['cleaned_content']:\n",
    "#     print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "\n",
    "For unqie vocabulary before and after processing: \n",
    "I started by concatenating the \"content\" column into a single string, and then divided it into individual words using split(). Then, I converted it into a set of unique elements and calculated the length of this set.\n",
    "\n",
    "I found the following: \n",
    "\n",
    "Vocabulary before processing (number of unique words): 30005\n",
    "\n",
    "Vocabulary after processing: (number of unique words): 25686\n",
    "\n",
    "The unique vocabulary is reduced by approximately 4.300 after processing. This is equal to a reduction rate of around 14.39 %. \n",
    "\n",
    "In order to find word frequency, I inserted every word into a dictionary containing the word as key and its appearance as value (+ 1 for every appearance in the for loop). \n",
    "To sort word frequency, I used the \"sorted\" method and a lambda function that ordered words by frequency in descending order.\n",
    "\n",
    "Finally, I plotted the top 50 most frequent words. I found that a lot of these words were simple stop words like \"the\", \"of\" and \"to\", which would have been sorted out in a more rigorous analysis. \n",
    "\n",
    "However, if we would look from around frequency 150 and below, we see a number of interesting words such as \"president\", \"bitcoin\", \"stocks\", \"trump\", \"global\", \"obama\", \"free\", \"research\", etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 16569: expected str instance, float found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Total vocabulary size before processing\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m total_vocab_before \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Total unique vocabulary size before processing\u001b[39;00m\n\u001b[1;32m      7\u001b[0m unique_vocab_before \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(total_vocab_before))\n",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 16569: expected str instance, float found"
     ]
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Total vocabulary size before processing\n",
    "total_vocab_before = ' '.join(data['content']).split()\n",
    "\n",
    "# Total unique vocabulary size before processing\n",
    "unique_vocab_before = len(set(total_vocab_before))\n",
    "\n",
    "# Clean data: apply the clean_text function\n",
    "data['cleaned_content'] = data['content'].apply(clean_text)\n",
    "\n",
    "# Split into separate words and calculate the total vocabulary\n",
    "total_vocab_after = ' '.join(data['cleaned_content']).split()\n",
    "\n",
    "# Calculate size of unique vocabulary after cleaning\n",
    "unique_vocab_after = len(set(total_vocab_after))\n",
    "\n",
    "print(f\"Vocabulary before processing (number of unique words): {unique_vocab_before}\")\n",
    "print(f\"Vocabulary after processing: (number of unique words): {unique_vocab_after}\")\n",
    "print(f\"Number of words removed: {unique_vocab_before - unique_vocab_after}\")\n",
    "print(f\"Reduction rate: {(unique_vocab_before - unique_vocab_after) / unique_vocab_before * 100:.2f} %\")\n",
    "\n",
    "# Calculate word frequency\n",
    "def word_frequency(data):\n",
    "    word_freq = {}\n",
    "    for word in data:\n",
    "        word_freq[word] = word_freq.get(word, 0) + 1\n",
    "    return word_freq\n",
    "\n",
    "# Sort the vocabulary by frequency\n",
    "def freq_sort(data):\n",
    "    return dict(sorted(data.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "sorted_top_vocab = freq_sort(word_frequency(total_vocab_after))\n",
    "\n",
    "print(sorted_top_vocab) # Print the sorted vocabulary\n",
    "\n",
    "def plot_top_words(data):\n",
    "    # Get the 50 most common words\n",
    "    top_50 = list(sorted_top_vocab.items())[:50]\n",
    "\n",
    "    # Separate words and frequencies\n",
    "    words, frequencies = zip(*top_50)\n",
    "\n",
    "    # Plot the bar chart\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.bar(words, frequencies, color='skyblue')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Top 50 Most Frequent Words')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_top_words(sorted_top_vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
